% !TeX program = lualatex
% !TeX encoding = UTF-8
% !TeX root = ./thesis.tex

\section{Higher Order Logic Programming}

At the beginning of our work it was decided that the this project should be implemented using $\lambda$Prolog and ELPI, one of its implementations which allows for more flexibility (\eg optional type declarations) more functionality (\eg constraint programming, which will be useful in later phases of the project) and also improved performances compared to other major implementations of $\lambda$Prolog.

As a programming language $\lambda$Prolog is naturally a good fit for a binder-heavy project like our as it natively offer all the functionality required to handle $\alpha$-equivalence, capture avoiding substitutions and higher-order unification; moreover the high level abstractions and backtracking allow for almost a 1-1 encoding of inference rules. As an example  below we can see the comparison between a natural deduction inference rule and a $\lambda$Prolog clause in the case of $\Pi$-Introduction.
In natural deduction style we have the following inference rule
\[
\begin{prooftree}
\hypo{C(x)\ set\ [x \in B]}
\hypo{B\ set}
\infer{2}[\ensuremath{\Pi}-F]{\Pi_{x\in B}{C(x)}\ set}
\end{prooftree}\]
while in $\lambda$Prolog we would write
\begin{verbatim}
ofType (setPi B C) KIND3 IE
    :- ofType B KIND1 IE
    ,  (pi x\ locDecl x B
        => ofType (C x) KIND2 IE)
    ,  pts_fun KIND1 KIND2 KIND3
    .
\end{verbatim}
where \verb|ofType| represent the $set$ judgement, \verbl{locDecl} is used to locally introduce hypothesis in the context and \verb|pts_fun| is used to decide whether the produced type is a set or a collection. 

While the $\lambda$Prolog version is more verbose it should be noted that this one rule encode the $\Pi$-Formation rule for both the extensional and the intensional level and also for all combination of arguments kinds between sets, collections, propositions and small proposition.

\subsection{Changes to the calculus}
As pointed out in the introduction before we could begin the implementation we had to apply some changes to the calculi exposed in \cite{maietti2009minimalist}.

	\subsubsection{Syntax Directed Rules}
	Between the rules of \emph{mTT} and \emph{emTT} there is a rule which states that equal type have the same elements, that is:
	\[
	\begin{prooftree}
	\hypo{a \in A}
	\hypo{A = B}
	\infer2[conv]{a\in B}
	\end{prooftree}
	\]
 	
 	This rule is problematic from an implementation point of view since it can be triggered at any moment and (at the extensional level) would start a proof search trying to prove $A=B$ for a unknown $A$. A simple solution is to preemptively compose this rule to all other rule that might need it; for example we can see that the elimination rule for dependents products changes from 
 	\[
 	\begin{prooftree}
	\hypo{f\in \Pi_{x\in B}{C(x)}}
	\hypo{t \in B}
	\infer2[\ensuremath{\Pi}-E]{\s{apply}(f,t)\in C(t)}
 	\end{prooftree}
 	\]
 	to 
 	\[
 	\begin{prooftree}
 	\hypo{f\in \Pi_{x\in B}{C(x)}}
 	\hypo{t \in B'}
 	\hypo{B=B'}
 	\infer3[\ensuremath{\Pi}-E]{\s{apply}(f,t)\in C(t)}
 	\end{prooftree}
 	\]
 	 the main difference being that in the syntax directed version we check that the domain type of $f$ and the inferred type of $t$ are convertible instead of requiring syntactic equality like in the original rule. In this way the power of the calculus is surely not increased as the syntax directed rule is admissible in the calculus with the two previous rules, indeed
 	\[\begin{prooftree}
 	\hypo{f\in \Pi_{x\in B}{C(x)}}
 	\hypo{t \in B'}
 	\hypo{B=B'}
 	\infer2[conv]{t \in B}
 	\infer2[\ensuremath{\Pi}-E]{\s{apply}(f,t)\in C(t)}
 	\end{prooftree}\]
 	is a derivation of the same judgement from the same hypothesis.
 	
 	For this change to be effective we also need to reformulate the conversion rules given by the calculi (\eg $\s{apply}(\lambda^B x. f,t) = f\{t/x\} $) in directed rules describing a reduction predicate $\rhd$ (\eg  $\s{apply}(\lambda^B x. f,t) \rhd f\{t/x\} $) from which we will define equality as reducibility to a common term, which is equivalent to the original equality given that both calculi of the Minimalist Foundation have both the property of Church-Rosser confluence and strong normalization. This new relation will be implemented by the predicate \verb|conv|.
 	
 	The full modifications to the calculi in our implementation with respect to making the rule syntax directed are mostly about checking for convertibility/equality where mTT/emTT require syntactical equality. We plan to provide a full proof of the validity of this modification, and also of the followings, in Abella in the future.
	
	

\subsubsection{Informative Proof-terms at the Extensional Level and Exstensional Equality}
	\[\begin{prooftree}
	\hypo{b\in B}
	\hypo{\s{true} \in C(b)}
	\hypo{C(x)\ prop\ [x \in B]}
	\infer3[\ensuremath{\exists}-\ensuremath{\text{I}_{ext}}]{\s{true} \in \exists_{x\in B}{C(x)}}
	\end{prooftree}\quad
	\begin{prooftree}
	\hypo{\s{true}\in \s{Eq}(A,a,b)}
	\infer1[Eq-E]{a = b \in A}
	\end{prooftree}\]\[
	\begin{prooftree}
	\hypo{b\in B}
	\hypo{p \in C(b)}
	\hypo{C(x)\ prop\ [x \in B]}
	\infer3[\ensuremath{\exists}-\ensuremath{\text{I}_{int}}]{<b,_{\exists}\  p> \in \exists_{x\in B}{C(x)}}
	\end{prooftree}
	\]
	In the calculus emTT a token proof term $\s{true}$ is used to represent all proofs of propositions and small propositions; this poses problems during the interpretation when we need to construct intensional proof terms given only the token $\s{true}$. The naive approach would be to attempt a proof search at the intensional level; this might be feasible since (if the proposition was type checked at the extensional level) intensional convertibility is decidable and we know from Maietti's proofs \cite{maietti2009minimalist} that a suitable proof terms exists (by the proof irrelevance property of mTT which particular proof term is chosen does not matter). This approach was not even considered for implementation.
	
	A first more feasible solution might be to typecheck
	